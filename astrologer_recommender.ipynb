{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "QUnxO0gSnQHZ",
        "5crrwCsnM5JX",
        "L7YYz9zXkFWY",
        "jpsH-_ZNOlzB",
        "ke1U_Z7ekmCM",
        "znYouPndjdAc"
      ],
      "authorship_tag": "ABX9TyOFQc1ItQ91Tbl+Lt9hLcuI"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "QUnxO0gSnQHZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xW356P9AKuEB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f2234c5-6dcf-4518-810f-3937430aa656"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           name                        tags     score\n",
            "0  Aarav Sharma   career, finance, business  0.476144\n",
            "2   Rohan Mehta    education, study, career  0.383065\n",
            "4   Kabir Singh  health, well-being, stress  0.255081\n"
          ]
        }
      ],
      "source": [
        "# ‚úÖ Install necessary libraries (only once)\n",
        "!pip install -q sentence-transformers scikit-learn\n",
        "\n",
        "# Imports\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "# Load the embedding model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Mock astrologer data\n",
        "astrologers = [\n",
        "    {\"name\": \"Aarav Sharma\", \"tags\": \"career, finance, business\"},\n",
        "    {\"name\": \"Diya Kapoor\", \"tags\": \"love, relationships, marriage\"},\n",
        "    {\"name\": \"Rohan Mehta\", \"tags\": \"education, study, career\"},\n",
        "    {\"name\": \"Meera Joshi\", \"tags\": \"spirituality, peace, life purpose\"},\n",
        "    {\"name\": \"Kabir Singh\", \"tags\": \"health, well-being, stress\"},\n",
        "]\n",
        "df_astrologers = pd.DataFrame(astrologers)\n",
        "\n",
        "# User input (chat or profile text)\n",
        "user_input = \"I‚Äôm going through a tough phase in my career and need guidance about job stability and finances.\"\n",
        "\n",
        "# Compute embeddings\n",
        "user_embedding = model.encode([user_input])\n",
        "astrologer_embeddings = model.encode(df_astrologers['tags'].tolist())\n",
        "\n",
        "# Compute cosine similarity\n",
        "similarities = cosine_similarity(user_embedding, astrologer_embeddings)[0]\n",
        "df_astrologers['score'] = similarities\n",
        "\n",
        "# Show top 3 astrologers\n",
        "top_3 = df_astrologers.sort_values(by='score', ascending=False).head(3)\n",
        "print(top_3[['name', 'tags', 'score']])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulated user inputs\n",
        "user_inputs = [\n",
        "    \"Lately I've been having trouble in my relationship. I feel distant from my partner and unsure about the future of our love life.\",\n",
        "    \"I'm confused about what to study next. I want to make the right choice for my academic and career goals.\",\n",
        "    \"I feel lost and unfulfilled lately. I want to understand my purpose and seek direction.\",\n",
        "    \"I‚Äôve been thinking about starting a business but unsure if it‚Äôs the right time. I need insight into financial risks and future success.\",\n",
        "    \"I've been under a lot of stress lately, and my health is taking a hit. I want guidance on how to improve my well-being.\",\n",
        "    \"I'm facing anxiety about my job and love life. I need clarity about both career direction and my current partner.\"\n",
        "]\n",
        "\n",
        "# Run recommendations for each user input\n",
        "for i, user_input in enumerate(user_inputs, start=1):\n",
        "    print(f\"\\n=== User {i} ===\")\n",
        "    print(f\"Input: {user_input}\\n\")\n",
        "\n",
        "    # Embed user input\n",
        "    user_embedding = model.encode([user_input])\n",
        "\n",
        "    # Recompute similarity with astrologer tags\n",
        "    similarities = cosine_similarity(user_embedding, astrologer_embeddings)[0]\n",
        "    df_astrologers['score'] = similarities\n",
        "\n",
        "    # Sort and show top 3\n",
        "    top_matches = df_astrologers.sort_values(by='score', ascending=False).head(3)\n",
        "    print(\"Top astrologer matches:\")\n",
        "    print(top_matches[['name', 'tags', 'score']].to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2T34iIHVHH2y",
        "outputId": "c6543410-cfbd-407e-a21d-f755b0e18d82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== User 1 ===\n",
            "Input: Lately I've been having trouble in my relationship. I feel distant from my partner and unsure about the future of our love life.\n",
            "\n",
            "Top astrologer matches:\n",
            "        name                          tags    score\n",
            " Diya Kapoor love, relationships, marriage 0.227898\n",
            " Kabir Singh    health, well-being, stress 0.207819\n",
            "Aarav Sharma     career, finance, business 0.125473\n",
            "\n",
            "=== User 2 ===\n",
            "Input: I'm confused about what to study next. I want to make the right choice for my academic and career goals.\n",
            "\n",
            "Top astrologer matches:\n",
            "        name                              tags    score\n",
            " Rohan Mehta          education, study, career 0.615873\n",
            "Aarav Sharma         career, finance, business 0.513287\n",
            " Meera Joshi spirituality, peace, life purpose 0.232154\n",
            "\n",
            "=== User 3 ===\n",
            "Input: I feel lost and unfulfilled lately. I want to understand my purpose and seek direction.\n",
            "\n",
            "Top astrologer matches:\n",
            "        name                              tags    score\n",
            " Meera Joshi spirituality, peace, life purpose 0.408275\n",
            " Rohan Mehta          education, study, career 0.324704\n",
            "Aarav Sharma         career, finance, business 0.285046\n",
            "\n",
            "=== User 4 ===\n",
            "Input: I‚Äôve been thinking about starting a business but unsure if it‚Äôs the right time. I need insight into financial risks and future success.\n",
            "\n",
            "Top astrologer matches:\n",
            "        name                       tags    score\n",
            "Aarav Sharma  career, finance, business 0.322520\n",
            " Rohan Mehta   education, study, career 0.198167\n",
            " Kabir Singh health, well-being, stress 0.160356\n",
            "\n",
            "=== User 5 ===\n",
            "Input: I've been under a lot of stress lately, and my health is taking a hit. I want guidance on how to improve my well-being.\n",
            "\n",
            "Top astrologer matches:\n",
            "       name                              tags    score\n",
            "Kabir Singh        health, well-being, stress 0.514512\n",
            "Meera Joshi spirituality, peace, life purpose 0.238853\n",
            "Diya Kapoor     love, relationships, marriage 0.088576\n",
            "\n",
            "=== User 6 ===\n",
            "Input: I'm facing anxiety about my job and love life. I need clarity about both career direction and my current partner.\n",
            "\n",
            "Top astrologer matches:\n",
            "        name                       tags    score\n",
            "Aarav Sharma  career, finance, business 0.349740\n",
            " Kabir Singh health, well-being, stress 0.342059\n",
            " Rohan Mehta   education, study, career 0.320448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Astrologer Recommendation Engine ‚Äì Streamlit (Colab Version)\n",
        "- This interactive app runs inside Google Colab using streamlit and is embedded as an iframe.\n",
        "- It starts a local server on port `8501` and serves the UI directly.\n",
        "- The Streamlit UI will run in the output of this notebook.\n",
        "\n",
        "As I primarily work on Google Colab, I made a few minimal adjustments to run Streamlit via an iframe, since local servers aren't directly accessible in the Colab environment.\n",
        "\n",
        "It checks for any process already using port `8501` on the local host, terminates it if found, and then starts a fresh Streamlit instance on the same port.\n",
        "\n",
        "Upload `streamlit_astrologer_app.py` to the environment"
      ],
      "metadata": {
        "id": "5crrwCsnM5JX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit\n",
        "import streamlit as st\n",
        "from google.colab import output\n",
        "import subprocess\n",
        "import time\n",
        "import socket\n",
        "import os\n",
        "import psutil # A cross-platform library for retrieving running processes\n",
        "\n",
        "app_file_name = \"/content/streamlit_astrologer_app.py\"\n",
        "\n",
        "# --- 2. Define the port Streamlit will run on ---\n",
        "STREAMLIT_PORT = 8501\n",
        "\n",
        "# --- 3. Function to check if a port is open ---\n",
        "def is_port_open(port, host='127.0.0.1'):\n",
        "    \"\"\"Checks if a given port on a host is open.\"\"\"\n",
        "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "    try:\n",
        "        s.settimeout(1) # Set a timeout for the connection attempt\n",
        "        s.connect((host, port))\n",
        "        return True\n",
        "    except (socket.error, socket.timeout):\n",
        "        return False\n",
        "    finally:\n",
        "        s.close()\n",
        "\n",
        "# --- 4. Function to kill processes using a specific port ---\n",
        "def kill_process_on_port(port):\n",
        "    \"\"\"Finds and kills processes listening on the given port.\"\"\"\n",
        "    print(f\"Checking for processes on port {port} to terminate...\")\n",
        "    for proc in psutil.process_iter(['pid', 'name', 'connections']):\n",
        "        try:\n",
        "            for conn in proc.connections(kind='inet'):\n",
        "                if conn.laddr.port == port:\n",
        "                    print(f\"Found process {proc.name()} (PID: {proc.pid}) listening on port {port}. Terminating...\")\n",
        "                    proc.terminate() # or proc.kill() for a more forceful kill\n",
        "                    time.sleep(0.1) # Give it a moment to terminate\n",
        "                    if proc.is_running():\n",
        "                        print(f\"Process {proc.pid} still running, forcing kill.\")\n",
        "                        proc.kill()\n",
        "                    print(f\"Process {proc.pid} terminated.\")\n",
        "                    return True # Found and terminated one process\n",
        "        except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):\n",
        "            # Process no longer exists or access denied (e.g., system process)\n",
        "            pass\n",
        "    print(f\"No active processes found on port {port}.\")\n",
        "    return False\n",
        "\n",
        "# --- 5. Kill any existing Streamlit process on the port before starting a new one ---\n",
        "kill_process_on_port(STREAMLIT_PORT)\n",
        "time.sleep(2)\n",
        "\n",
        "# --- 6. Start Streamlit in the background ---\n",
        "print(f\"Starting Streamlit app '{app_file_name}' on port {STREAMLIT_PORT} in the background...\")\n",
        "\n",
        "streamlit_command = [\n",
        "    \"streamlit\", \"run\", app_file_name,\n",
        "    \"--server.port\", str(STREAMLIT_PORT),\n",
        "    \"--server.enableCORS\", \"false\",\n",
        "    \"--server.enableXsrfProtection\", \"false\"\n",
        "]\n",
        "\n",
        "# Start the process.\n",
        "# preexec_fn=os.setsid detaches the child process from the parent,\n",
        "# allowing it to run independently even if the parent cell finishes.\n",
        "streamlit_process = subprocess.Popen(\n",
        "    streamlit_command,\n",
        "    stdout=subprocess.DEVNULL, # Redirect stdout to /dev/null\n",
        "    stderr=subprocess.DEVNULL, # Redirect stderr to /dev/null\n",
        "    preexec_fn=os.setsid\n",
        ")\n",
        "\n",
        "print(f\"Streamlit process started with PID: {streamlit_process.pid}\")\n",
        "\n",
        "# --- 7. Wait for Streamlit to become available ---\n",
        "max_retries = 30 # Try for up to 30 seconds\n",
        "retry_delay = 1 # Check every 1 second\n",
        "\n",
        "print(f\"Waiting for Streamlit app to become available on port {STREAMLIT_PORT}...\")\n",
        "for i in range(max_retries):\n",
        "    if is_port_open(STREAMLIT_PORT):\n",
        "        print(f\"Streamlit app is now accessible on port {STREAMLIT_PORT}!\")\n",
        "        break\n",
        "    print(f\"Attempt {i+1}/{max_retries}: Port {STREAMLIT_PORT} not open yet. Retrying in {retry_delay}s...\")\n",
        "    time.sleep(retry_delay)\n",
        "else:\n",
        "    print(\"Error: Streamlit app did not become available within the expected time.\")\n",
        "    print(\"Please check the Colab logs or try restarting the runtime if the issue persists.\")\n",
        "    # Attempt to terminate the process if it failed to start\n",
        "    streamlit_process.terminate()\n",
        "    streamlit_process.wait()\n",
        "    raise RuntimeError(\"Streamlit server failed to start or become accessible.\")\n",
        "\n",
        "# --- 8. Serve the Streamlit app as an iframe ---\n",
        "print(\"Serving Streamlit app in iframe...\")\n",
        "output.serve_kernel_port_as_iframe(STREAMLIT_PORT, height=600)\n",
        "\n",
        "print(\"Cell execution finished. Streamlit app should be visible and interactive above.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for processes on port 8501 to terminate...\n",
            "Found process streamlit (PID: 7238) listening on port 8501. Terminating...\n",
            "Process 7238 still running, forcing kill.\n",
            "Process 7238 terminated.\n",
            "Starting Streamlit app '/content/streamlit_astrologer_app.py' on port 8501 in the background...\n",
            "Streamlit process started with PID: 7785\n",
            "Waiting for Streamlit app to become available on port 8501...\n",
            "Attempt 1/30: Port 8501 not open yet. Retrying in 1s...\n",
            "Streamlit app is now accessible on port 8501!\n",
            "Serving Streamlit app in iframe...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "    const iframe = document.createElement('iframe');\n",
              "    iframe.src = new URL(path, url).toString();\n",
              "    iframe.height = height;\n",
              "    iframe.width = width;\n",
              "    iframe.style.border = 0;\n",
              "    iframe.allow = [\n",
              "        'accelerometer',\n",
              "        'autoplay',\n",
              "        'camera',\n",
              "        'clipboard-read',\n",
              "        'clipboard-write',\n",
              "        'gyroscope',\n",
              "        'magnetometer',\n",
              "        'microphone',\n",
              "        'serial',\n",
              "        'usb',\n",
              "        'xr-spatial-tracking',\n",
              "    ].join('; ');\n",
              "    element.appendChild(iframe);\n",
              "  })(8501, \"/\", \"100%\", 600, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cell execution finished. Streamlit app should be visible and interactive above.\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "id": "Z-4n1jpQ33p_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 825
        },
        "collapsed": true,
        "outputId": "7c885cc2-ceb8-476e-9131-8c0f8bb8b831"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process"
      ],
      "metadata": {
        "id": "L7YYz9zXkFWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Kill the process manually\n",
        "\n",
        "!kill {streamlit_process.pid}"
      ],
      "metadata": {
        "id": "0vIHJTxlMZhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check background processes\n",
        "\n",
        "import psutil\n",
        "\n",
        "print(\"Listing Python-related background processes:\")\n",
        "for proc in psutil.process_iter(['pid', 'name', 'cmdline']):\n",
        "    try:\n",
        "        # Check if the process name or command line contains \"python\" or \"streamlit\"\n",
        "        if \"python\" in proc.name().lower() or (proc.cmdline() and \"streamlit\" in \" \".join(proc.cmdline()).lower()):\n",
        "            print(f\"PID: {proc.pid}, Name: {proc.name()}, Command: {' '.join(proc.cmdline())}\")\n",
        "    except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):\n",
        "        # Handle cases where a process might disappear or access is denied\n",
        "        pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZ2-DLctlVIa",
        "outputId": "59011f4d-fbdd-4b13-f959-88ab67d1c8e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listing Python-related background processes:\n",
            "PID: 64, Name: python3, Command: \n",
            "PID: 870, Name: python3, Command: /usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-c1b85791-cd5c-4b99-84f8-cd08150645ba.json\n",
            "PID: 898, Name: python3, Command: /usr/bin/python3 /usr/local/lib/python3.11/dist-packages/debugpy/adapter --for-server 42045 --host 127.0.0.1 --port 40943 --server-access-token a9694967d8ddb87d931e0a88d8db71aae43e958843b2bc1ec6854b39a7e82f6a\n",
            "PID: 2857, Name: python3, Command: python3 /opt/google/drive/drive-filter.py\n",
            "PID: 14511, Name: streamlit, Command: /usr/bin/python3 /usr/local/bin/streamlit run /content/astrologer_engine.py --server.port 8501 --server.enableCORS false --server.enableXsrfProtection false\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initializing Ollama"
      ],
      "metadata": {
        "id": "jpsH-_ZNOlzB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The code first initializes your drive.\n",
        "- The code then installs gemma:2b model in your drive and initializes Ollama on a seperate thread"
      ],
      "metadata": {
        "id": "mw1a-jIukCZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install ollama\n",
        "import os\n",
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "from google.colab import drive\n",
        "import ollama\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/ollama_models'  # Insert the folder path where you want to install ollama models\n",
        "print(\"\\n\\n--- Initializing Ollama Environment ---\")\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "print(\"\\n1. Mounting Google Drive...\")\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=True) # Use force_remount=True for reliability on restart\n",
        "    print(\"Google Drive mounted successfully.\\nChanging current working directory to Langchain folder\")\n",
        "    os.chdir('/content/drive/MyDrive/LangChain')\n",
        "    print(f'Current working directory changed to {os.getcwd()}')\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}\")\n",
        "    print(\"Please ensure you authorize Google Drive access.\")\n",
        "    # Exit or raise an error if mounting fails, as subsequent steps depend on it\n",
        "    raise\n",
        "\n",
        "# Define the path on Google Drive where models will be stored\n",
        "OLLAMA_MODELS_DIR = folder_path\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "print(f\"\\n2. Ensuring Ollama models directory exists: {OLLAMA_MODELS_DIR}\")\n",
        "! mkdir -p {OLLAMA_MODELS_DIR}\n",
        "\n",
        "# Set the OLLAMA_MODELS environment variable\n",
        "os.environ['OLLAMA_MODELS'] = OLLAMA_MODELS_DIR\n",
        "print(f\"OLLAMA_MODELS set to: {os.environ['OLLAMA_MODELS']}\")\n",
        "\n",
        "# Also set OLLAMA_HOST and OLLAMA_ORIGINS for Colab access\n",
        "os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "print(\"Ollama host and origins environment variables set.\")\n",
        "\n",
        "# Function to run the Ollama server\n",
        "def run_ollama_serve():\n",
        "    print(\"Starting Ollama server process...\")\n",
        "    # Using a try-except block here to catch issues with 'ollama serve' itself\n",
        "    try:\n",
        "        # Use shell=True for `subprocess.Popen` when running shell commands\n",
        "        # And ensure it's running in background\n",
        "        subprocess.Popen(\"ollama serve\", shell=True, preexec_fn=os.setsid, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        print(\"Ollama server process initiated.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: 'ollama' command not found. Ollama may not be installed correctly.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while starting Ollama server: {e}\")\n",
        "\n",
        "# 3. Install Ollama (if not already installed in the current session)\n",
        "print(\"\\n3. Installing Ollama...\")\n",
        "# -fSL ensures it follows redirects and fails silently on HTTP errors, shows progress with -L\n",
        "try:\n",
        "    # Use a non-interactive installation to prevent prompts if any\n",
        "    ! curl -fsSL https://ollama.ai/install.sh | sh\n",
        "    print(\"Ollama installation command executed. (Note: May not show live progress in Colab output immediately)\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during Ollama installation: {e}\")\n",
        "    print(\"Please check the Colab output for details on why the curl command failed.\")\n",
        "    raise\n",
        "\n",
        "# 4. Start the Ollama server in a new thread\n",
        "print(\"\\n4. Starting Ollama server in background thread...\")\n",
        "ollama_thread = threading.Thread(target=run_ollama_serve)\n",
        "ollama_thread.daemon = True # Allows the main program to exit even if this thread is running\n",
        "ollama_thread.start()\n",
        "\n",
        "# Give the server a few seconds to initialize\n",
        "print(\"Waiting for Ollama server to initialize (10 seconds)...\")\n",
        "time.sleep(10) # Adjust as needed\n",
        "print(\"Ollama server should be running.\")\n",
        "\n",
        "# Pulling model llama 3\n",
        "# Adding a timeout for the pull command in case it hangs\n",
        "# 3600 seconds = 1 hour timeout. Adjust if you expect very slow downloads or large models.\n",
        "# !OLLAMA_TIMEOUT=3600 ollama pull llama3\n",
        "\n",
        "# 6. Verify models are listed\n",
        "print(\"\\n6. Listing installed Ollama models:\")\n",
        "! ollama list\n",
        "\n",
        "print(\"\\n--- Ollama Environment Initialization Complete! ---\")\n",
        "print(\"You can now proceed with your model interactions.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nt2YMUrpOo1f",
        "outputId": "b3b90a8f-36ff-494e-a6bb-f9a4f629585b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ollama in /usr/local/lib/python3.11/dist-packages (0.5.1)\n",
            "Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.11/dist-packages (from ollama) (0.28.1)\n",
            "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.11/dist-packages (from ollama) (2.11.7)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27->ollama) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27->ollama) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27->ollama) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27->ollama) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->ollama) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->ollama) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9->ollama) (0.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27->ollama) (1.3.1)\n",
            "\n",
            "\n",
            "--- Initializing Ollama Environment ---\n",
            "\n",
            "1. Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully.\n",
            "Changing current working directory to Langchain folder\n",
            "Current working directory changed to /content/drive/MyDrive/LangChain\n",
            "\n",
            "2. Ensuring Ollama models directory exists: /content/drive/MyDrive/ollama_models\n",
            "OLLAMA_MODELS set to: /content/drive/MyDrive/ollama_models\n",
            "Ollama host and origins environment variables set.\n",
            "\n",
            "3. Installing Ollama...\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "Ollama installation command executed. (Note: May not show live progress in Colab output immediately)\n",
            "\n",
            "4. Starting Ollama server in background thread...\n",
            "Starting Ollama server process...\n",
            "Waiting for Ollama server to initialize (10 seconds)...\n",
            "Ollama server process initiated.\n",
            "Ollama server should be running.\n",
            "Error: accepts 1 arg(s), received 2\n",
            "\n",
            "6. Listing installed Ollama models:\n",
            "NAME                        ID              SIZE      MODIFIED   \n",
            "mxbai-embed-large:latest    468836162de7    669 MB    8 days ago    \n",
            "\n",
            "--- Ollama Environment Initialization Complete! ---\n",
            "You can now proceed with your model interactions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat prompt template approach\n",
        "The gemma model will act as an astrolger and wll respond accordingly."
      ],
      "metadata": {
        "id": "ke1U_Z7ekmCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the prompt\n",
        "try:\n",
        "    # Start an interactive chat session\n",
        "    print(\"\\n--- üîÆ Interactive Chat Session ‚Äì AI Astrologer (Type 'exit' to quit) ---\")\n",
        "\n",
        "    # Add system prompt as chat template\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": (\n",
        "                \"You are a wise and thoughtful AI astrologer. \"\n",
        "                \"The user will provide palm readings or horoscope summaries. \"\n",
        "                \"Reply with gentle, insightful life advice in 2‚Äì3 sentences. \"\n",
        "                \"Keep your tone mystical, encouraging, and grounded in astrology themes.\"\n",
        "            )\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You üåô: \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        messages.append({'role': 'user', 'content': user_input})\n",
        "\n",
        "        stream_response = ollama.chat(model='gemma:2b', messages=messages, stream=True)\n",
        "\n",
        "        ai_response_content = \"\"\n",
        "        print(\"üîÆ Gemma 2B:\", end=\" \")\n",
        "        for chunk in stream_response:\n",
        "            if 'content' in chunk['message']:\n",
        "                print(chunk['message']['content'], end=\"\", flush=True)\n",
        "                ai_response_content += chunk['message']['content']\n",
        "        print()  # Newline after streamed reply\n",
        "\n",
        "        messages.append({'role': 'assistant', 'content': ai_response_content})\n",
        "\n",
        "except ollama.ResponseError as e:\n",
        "    print(f\"\\n‚ùå Ollama Error: {e}\")\n",
        "    print(\"Make sure the Ollama server is running and the model is pulled locally.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ö†Ô∏è Unexpected error: {e}\")\n",
        "\n",
        "print(\"\\nüåå Chat session ended.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZKaBVoRiB-5",
        "outputId": "a93df695-55d0-44f5-ff3a-282276dbeb86"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- üîÆ Interactive Chat Session ‚Äì AI Astrologer (Type 'exit' to quit) ---\n",
            "You üåô: Hello\n",
            "üîÆ Gemma 2B: Greetings, dear user. I am here to offer guidance and insights gleaned from the cosmic dance of planets and stars. Let your palm reveal the whispers of the past and the secrets of the future, for they speak of a path paved with both wisdom and wonder.\n",
            "You üåô: It works now\n",
            "üîÆ Gemma 2B: The stars whisper secrets of a love that transcends time and space. Trust in the cycles of life and let your heart guide you toward a fulfilling and harmonious destiny.\n",
            "You üåô: exit\n",
            "\n",
            "üåå Chat session ended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streamlit astrologer Engine"
      ],
      "metadata": {
        "id": "znYouPndjdAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q dotenv\n",
        "!pip install -q langchain_community\n",
        "!pip install -q streamlit\n",
        "import streamlit as st\n",
        "from google.colab import output\n",
        "import subprocess\n",
        "import time\n",
        "import socket\n",
        "import os\n",
        "import psutil # A cross-platform library for retrieving running processes\n",
        "\n",
        "app_file_name = \"/content/astrologer_engine.py\"\n",
        "\n",
        "# --- 2. Define the port Streamlit will run on ---\n",
        "STREAMLIT_PORT = 8501\n",
        "\n",
        "# --- 3. Function to check if a port is open ---\n",
        "def is_port_open(port, host='127.0.0.1'):\n",
        "    \"\"\"Checks if a given port on a host is open.\"\"\"\n",
        "    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "    try:\n",
        "        s.settimeout(1) # Set a timeout for the connection attempt\n",
        "        s.connect((host, port))\n",
        "        return True\n",
        "    except (socket.error, socket.timeout):\n",
        "        return False\n",
        "    finally:\n",
        "        s.close()\n",
        "\n",
        "# --- 4. Function to kill processes using a specific port ---\n",
        "def kill_process_on_port(port):\n",
        "    \"\"\"Finds and kills processes listening on the given port.\"\"\"\n",
        "    print(f\"Checking for processes on port {port} to terminate...\")\n",
        "    for proc in psutil.process_iter(['pid', 'name', 'connections']):\n",
        "        try:\n",
        "            for conn in proc.connections(kind='inet'):\n",
        "                if conn.laddr.port == port:\n",
        "                    print(f\"Found process {proc.name()} (PID: {proc.pid}) listening on port {port}. Terminating...\")\n",
        "                    proc.terminate() # or proc.kill() for a more forceful kill\n",
        "                    time.sleep(0.1) # Give it a moment to terminate\n",
        "                    if proc.is_running():\n",
        "                        print(f\"Process {proc.pid} still running, forcing kill.\")\n",
        "                        proc.kill()\n",
        "                    print(f\"Process {proc.pid} terminated.\")\n",
        "                    return True # Found and terminated one process\n",
        "        except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):\n",
        "            # Process no longer exists or access denied (e.g., system process)\n",
        "            pass\n",
        "    print(f\"No active processes found on port {port}.\")\n",
        "    return False\n",
        "\n",
        "# --- 5. Kill any existing Streamlit process on the port before starting a new one ---\n",
        "kill_process_on_port(STREAMLIT_PORT)\n",
        "time.sleep(2) # Give the port a moment to fully release after termination\n",
        "\n",
        "# --- 6. Start Streamlit in the background ---\n",
        "print(f\"Starting Streamlit app '{app_file_name}' on port {STREAMLIT_PORT} in the background...\")\n",
        "\n",
        "streamlit_command = [\n",
        "    \"streamlit\", \"run\", app_file_name,\n",
        "    \"--server.port\", str(STREAMLIT_PORT),\n",
        "    \"--server.enableCORS\", \"false\",\n",
        "    \"--server.enableXsrfProtection\", \"false\"\n",
        "]\n",
        "\n",
        "# Start the process.\n",
        "# preexec_fn=os.setsid detaches the child process from the parent,\n",
        "# allowing it to run independently even if the parent cell finishes.\n",
        "streamlit_process = subprocess.Popen(\n",
        "    streamlit_command,\n",
        "    stdout=subprocess.DEVNULL, # Redirect stdout to /dev/null\n",
        "    stderr=subprocess.DEVNULL, # Redirect stderr to /dev/null\n",
        "    preexec_fn=os.setsid\n",
        ")\n",
        "\n",
        "print(f\"Streamlit process started with PID: {streamlit_process.pid}\")\n",
        "\n",
        "# --- 7. Wait for Streamlit to become available ---\n",
        "max_retries = 30 # Try for up to 30 seconds\n",
        "retry_delay = 1 # Check every 1 second\n",
        "\n",
        "print(f\"Waiting for Streamlit app to become available on port {STREAMLIT_PORT}...\")\n",
        "for i in range(max_retries):\n",
        "    if is_port_open(STREAMLIT_PORT):\n",
        "        print(f\"Streamlit app is now accessible on port {STREAMLIT_PORT}!\")\n",
        "        break\n",
        "    print(f\"Attempt {i+1}/{max_retries}: Port {STREAMLIT_PORT} not open yet. Retrying in {retry_delay}s...\")\n",
        "    time.sleep(retry_delay)\n",
        "else:\n",
        "    print(\"Error: Streamlit app did not become available within the expected time.\")\n",
        "    print(\"Please check the Colab logs or try restarting the runtime if the issue persists.\")\n",
        "    # Attempt to terminate the process if it failed to start\n",
        "    streamlit_process.terminate()\n",
        "    streamlit_process.wait()\n",
        "    raise RuntimeError(\"Streamlit server failed to start or become accessible.\")\n",
        "\n",
        "# --- 8. Serve the Streamlit app as an iframe ---\n",
        "print(\"Serving Streamlit app in iframe...\")\n",
        "output.serve_kernel_port_as_iframe(STREAMLIT_PORT, height=600)\n",
        "\n",
        "print(\"Cell execution finished. Streamlit app should be visible and interactive above.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 788
        },
        "id": "rFeqjSMyjht5",
        "outputId": "5988b179-11a7-42a2-ae20-a2d65e8ede40"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for processes on port 8501 to terminate...\n",
            "No active processes found on port 8501.\n",
            "Starting Streamlit app '/content/astrologer_engine.py' on port 8501 in the background...\n",
            "Streamlit process started with PID: 20669\n",
            "Waiting for Streamlit app to become available on port 8501...\n",
            "Attempt 1/30: Port 8501 not open yet. Retrying in 1s...\n",
            "Streamlit app is now accessible on port 8501!\n",
            "Serving Streamlit app in iframe...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "    const iframe = document.createElement('iframe');\n",
              "    iframe.src = new URL(path, url).toString();\n",
              "    iframe.height = height;\n",
              "    iframe.width = width;\n",
              "    iframe.style.border = 0;\n",
              "    iframe.allow = [\n",
              "        'accelerometer',\n",
              "        'autoplay',\n",
              "        'camera',\n",
              "        'clipboard-read',\n",
              "        'clipboard-write',\n",
              "        'gyroscope',\n",
              "        'magnetometer',\n",
              "        'microphone',\n",
              "        'serial',\n",
              "        'usb',\n",
              "        'xr-spatial-tracking',\n",
              "    ].join('; ');\n",
              "    element.appendChild(iframe);\n",
              "  })(8501, \"/\", \"100%\", 600, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cell execution finished. Streamlit app should be visible and interactive above.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can manually kill the process if needed later by running:\n",
        "# !kill {streamlit_process.pid}"
      ],
      "metadata": {
        "id": "kz4dxts15Wak"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}